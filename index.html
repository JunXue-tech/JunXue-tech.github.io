
<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>Jun Xue - Homepage</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content="Jun Xue">
<meta property="og:title" content="Jun Xue">


  <link rel="canonical" href="https://junxue-tech.github.io//">
  <meta property="og:url" content="https://junxue-tech.github.io//">



  <meta property="og:description" content="">









<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="assets/css/main.css">

<meta http-equiv="cleartype" content="on">
<head>
  <base target="_blank">
</head>
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<link rel="manifest" href="images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-TESF26FDJS"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-TESF26FDJS');
    </script>

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="#about-me">Homepage</a></li>
          
            <li class="masthead__menu-item"><a href="/#about-me">About Me</a></li>
          
            <li class="masthead__menu-item"><a href="/#-news">News</a></li>
          
            <li class="masthead__menu-item"><a href="/#-honors-and-awards">Honors and Awards</a></li>
          
            <li class="masthead__menu-item"><a href="/#-research-activities">Research Activities</a></li>
          
            <li class="masthead__menu-item"><a href="/#-talks">Talks</a></li>
          
            <li class="masthead__menu-item"><a href="/#-experiences">Experiences</a></li>
          
            <li class="masthead__menu-item"><a href="/#-education">Education</a></li>
          
            <li class="masthead__menu-item"><a href="/#-publications">Publications</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="images/android-chrome-192x192.png" class="author__avatar" alt="Jun Xue">
  </div>

  <div class="author__content">
    <h3 class="author__name">Jun Xue</h3>
    <p class="author__bio">AI Speech Algorithm Engineer @ iFLYTEK</p>
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Hefei, China</li>
      
      
      
      
        <li><a href="junxue.tech@gmail.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
        <li><a href="https://www.researchgate.net/profile/Jun-Xue-18?ev=hdr_xprf"><i class="fab fa-fw fa-researchgate" aria-hidden="true"></i> ResearchGate</a></li>
      
      
      
        <li><a href="https://www.linkedin.com/in/%E5%86%9B-%E8%96%9B-78187b2a1/"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
      
      
      
      
        <li><a href="https://github.com/JunXue-tech"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
        <li><a href="https://scholar.google.com.hk/citations?user=YxoCHH4AAAAJ&hl=zh-CN"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
      <div class="author__urls_sm">
      
      
        <a href="junxue.tech@gmail.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
        <a href="https://www.researchgate.net/profile/Jun-Xue-18?ev=hdr_xprf"><i class="fab fa-fw fa-researchgate" aria-hidden="true"></i></a>
      
      
      
      
      
        <a href="https://www.linkedin.com/in/%E5%86%9B-%E8%96%9B-78187b2a1/"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
        <a href="https://github.com/JunXue-tech"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>
      
      
      
      
      
  
      
      
      
      
        <a href="https://scholar.google.com.hk/citations?user=YxoCHH4AAAAJ&hl=zh-CN"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
      
      
    </div>
  </div>
</div>

  
  </div>

    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            <p><span class="anchor" id="about-me"></span></p>

<p>I am a AI Speech Algorithm Engineer at the <a href="https://www.iflytek.com/">iFLYTEK Research</a>.</p>

<p>My research aims to move towards a new generation of human-computer interaction centered on humans. This new generation of interaction should not only involve the exchange of information, but also achieve breakthroughs in emotion, cognition, and form.</p>

<p>During my master's studies, I conducted research on fake speech detection at the <a href="https://iiphci.ahu.edu.cn/">IIP-HCI Lab</a> of Anhui University, under the supervision of <a href="https://cs.ahu.edu.cn/2022/0305/c20806a280745/page.htm">Prof. Zhao Lü</a> and  <a href="https://fchest.github.io/">Dr. Cunhang Fan</a>.</p>

<h1 id="-news">🔥 News</h1>
<ul>
  <li><em>07.2024</em>, One paper accepted to IEEE Signal Processing Letters. <a href="https://doi.org/10.1109/LSP.2024.3431936">Dynamic ensemble teacher-student distillation framework for light-weight fake audio detection</a></li>
  <li><em>07.2024</em>, One journal paper accepted to Neural Networks. <a href="https://arxiv.org/pdf/2308.09944">Spatial reconstructed local attention Res2Net with F0 subband for fake speech detection</a></li>
  <li><em>03.2024</em>, One conference paper accepted to AAAI 2024. <a href="https://doi.org/10.1609/aaai.v38i8.28680">Progressive distillation based on masked generation feature method for knowledge graph completion</a></li>
  <li><em>11.2023</em>, One journal paper accepted to Speech Communication. <a href="https://doi.org/10.1016/j.specom.2023.102988">Subband fusion of complex spectrogram for fake speech detection</a></li>
  <li><em>06.2023</em>, One conference paper accepted to ADD 2023. <a href="https://arxiv.org/pdf/2306.15389">Multi-perspective information fusion res2net with randomspecmix for fake speech detection</a></li>
  <li><em>06.2023</em>, One conference paper accepted to ICASSP 2023. <a href="https://arxiv.org/pdf/2303.01211">Learning from yourself: A self-distillation method for fake speech detection</a></li>
  <li><em>10.2022</em>, One conference paper accepted to Proceedings of the 1st international workshop on deepfake detection for audio multimedia. <a href="https://arxiv.org/pdf/2208.01214">Audio Deepfake Detection Based on a Combination of F0 Information and Real Plus Imaginary Spectrogram Features</a></li>
</ul>

<h1 id="-honors-and-awards">🎖 Honors and Awards</h1>
<ul>
  <li><em>2024</em>, Provincial Excellent Graduate Student of Anhui Province</li>
  <li><em>2023</em>, Selected for the Excellent Student Forum at NCMMSC</li>
  <li><em>2023</em>, National Scholarship</li>
</ul>

<h1 id="-research-activities">💻 Research Activities</h1>
<p><strong><em>- Organizing Committee -</em></strong></p>
<ul>
  <li><a href="https://sites.google.com/view/spandldeteriorate">Workshop on Multi-Biological Sensing Data for Speech and Language Deterioration Prediction</a> at ACM MM Asia 2024</li>
  <li><a href="https://sites.google.com/view/gensec-challenge">GenSEC (old name: GenASR) challenge</a> at SLT 2024</li>
  <li>Special Session at Interspeech 2024: <a href="https://sites.google.com/view/responsiblespeech/">Responsible Speech Foundation Models</a></li>
  <li><a href="https://sites.google.com/view/uk-sigmm">UK Special Interest Group in Speech-Based Multimodal Information Processing</a></li>
  <li><a href="https://conferences.inf.ed.ac.uk/ukspeech2022/">UK Speech 2022</a></li>
</ul>

<p><strong><em>- Program Committee -</em></strong></p>
<ul>
  <li>Interspeech Young Female Researchers in Speech Workshop <a href="https://sites.google.com/view/yfrsw-2024/">2024</a>, <a href="https://sites.google.com/view/yfrsw-2022/">2022</a></li>
  <li><a href="https://icmi.acm.org/2021/index.php?id=cfdc">ICMI 2021 Doctoral Consortium</a></li>
</ul>

<p><strong><em>- Journal Review -</em></strong></p>
<ul>
  <li>IEEE Transactions on Affective Computing (2)</li>
  <li>Computer Speech and Language (2)</li>
  <li>IEEE Transactions on Audio, Speech and Language Processing (1)</li>
  <li>Speech Communication (1)</li>
  <li>Computers in Human Behavior (1)</li>
  <li>Journal of Rehabilitation and Assistive Technologies Engineering (1)</li>
  <li>Pattern Analysis and Applications (1)</li>
  <li>Robotics and Autonomous Systems (1)</li>
</ul>

<p><strong><em>- Conference Review -</em></strong></p>
<ul>
  <li>ICASSP’23-25, Interspeech’23-25, ICME’25, IJCNN’25, ASRU’23, SLT’22-24, UK Speech’22</li>
  <li>Interspeech Young Female Researchers in Speech Workshop’24 &amp; 22</li>
  <li>CHI’23 Late-Breaking Work</li>
  <li>IJCLR’23 CogAI Workshop</li>
  <li>ICMI’21 Late-Breaking Report &amp; Doctoral Consortium</li>
  <li>HRI’20 Late-Breaking Report</li>
</ul>

<p><strong><em>- Organizations &amp; Communities -</em></strong></p>
<ul>
  <li>ACM, AAAC, ISCA, IEEE, IEEE Signal Processing Society, SIGCHI, UK Speech, UK-SIGMM, Alan Turing Institute</li>
</ul>

<h1 id="-talks">🎙 Talks</h1>
<ul>
  <li><em>12.2024</em>, “Opportunities and Challenges of Language Emotion in Real Applications: LLMs, Multimodal Incongruity, and Human-Robot Interaction”. Tsinghua Laboratory of Brain and Intelligence, Tsinghua University (host: Prof. Dan Zhang)</li>
  <li><em>12.2024</em>, “Multi-view Cognitive State Detection Based on Pre-trained Speech and Language Models”. Speech and Audio Technology Lab, Tsinghua University (host: Prof. Wei-Qiang Zhang)</li>
  <li><em>03.2024</em>, “Opportunities and Challenges of Speech Emotion Recognition in the Era of Foundation Models”. Center for Interdisciplinary Research in Language Sciences, University of Science and Technology of China (host: Prof. Jiahong Yuan)</li>
  <li><em>11.2020</em>, “Affective Human-Robot Interaction”. Cognitive Developmental Robotics Lab, University of Tokyo (host: Prof. Yukie Nagai)</li>
</ul>

<h1 id="-grants">💰 Grants</h1>
<ul>
  <li><em>06.2023</em>, “Development of A Human-Centric Elderly Driving Education System”, Co-Investigator, ¥800,000. Strategic Research Project “2023-SRP-06”, Research Organization of Information and Systems</li>
</ul>

<h1 id="-experiences">👔 Experiences</h1>
<p><strong><em>- Teaching -</em></strong></p>
<ul>
  <li>TA (Coursework marker), Automatic Speech Recognition, University of Edinburgh, 2023 &amp; 2024</li>
  <li>TA (Tutor, demonstrator, and project marker), System Design Project, University of Edinburgh, 2023</li>
  <li>TA (Coursework and exam marker), Machine Learning, University of Edinburgh, 2022 &amp; 2024</li>
</ul>

<p><strong><em>- Supervision -</em></strong></p>
<ul>
  <li>
    <p>Cross-lingual Speech Emotion Recognition and Speech Emotion Diarisation: A Comparative Study between Humans and Machines</p>

    <p>Zhichen Han, MSc dissertation 2024/25 (Distinction), University of Edinburgh</p>
  </li>
  <li>
    <p>Revisiting the Shared Suprasegmental Acoustics Between Emotional Speech and Song through Self-Supervised Learning Models</p>

    <p>Yujia Sun, MSc dissertation 2024/25 (Distinction), University of Edinburgh</p>
  </li>
  <li>
    <p>Layerwise Analysis of HuBERT Acoustic Word Embeddings in the Context of Speech Emotion Recognition</p>

    <p>Alexandra Saliba, MSc dissertation 2023/24 (Distinction), University of Edinburgh</p>
  </li>
  <li>
    <p>Hierarchical Cross-Modal Transformer and A Study of Cross-Modal Attention for Affective Computing</p>

    <p>Yaoting Wang, MSc dissertation 2022/23 (Distinction), University of Edinburgh</p>
  </li>
  <li>
    <p>A Cross-Domain Study of Crossmodal Attention Based Multimodal Emotion Recognition</p>

    <p>Junling Liu, MSc dissertation 2021/22, University of Edinburgh</p>
  </li>
</ul>

<p><strong><em>- Working -</em></strong></p>
<ul>
  <li>Research Intern, Microsoft Research Audio and Acoustics Group</li>
  <li>Researcher, Honda R&amp;D Innovation Lab</li>
  <li>R&amp;D Engineer, NTT Data R&amp;D headquarters</li>
  <li>Student Researcher, ERATO ISHIGURO Symbiotic HRI Project, ATR</li>
</ul>

<h1 id="-education">📖 Education</h1>
<ul>
  <li>Ph.D. Candidate, Informatics, University of Edinburgh</li>
  <li>M.Sc., Intelligence Science and Technology, Kyoto University</li>
  <li>B.Eng., Electronic and Information Engineering, Nanjing University of Posts and Telecommunications</li>
</ul>

<h1 id="-publications">📝 Publications</h1>

<p><strong><em>- Papers -</em></strong></p>
<ul>
  <li>See my <a href="https://scholar.google.com/citations?hl=en&amp;user=WYSrzUsAAAAJ&amp;view_op=list_works&amp;sortby=pubdate">Google Scholar</a></li>
</ul>

<p><strong><em>- Patents -</em></strong></p>
<ul>
  <li><a href="https://patents.google.com/patent/US11107464B2/en">Feeling estimation device, feeling estimation method, and storage medium</a>. US11107464B2, JP2020091302A, CN111341349A</li>
  <li><a href="https://patents.google.com/patent/US11443759B2/en">Information processing apparatus, information processing method, and storage medium</a>. US11443759B2, JP2021026130A, CN112349301A</li>
  <li><a href="https://patents.google.com/patent/US11710499B2/en">Information-processing device, vehicle, computer-readable storage medium, and information-processing method</a>. US11710499B2, JP2021124642A, CN113221933A</li>
</ul>

<p><strong><em>- Technical Reports -</em></strong></p>
<ul>
  <li>
    <p>Crossmodal ASR Error Correction with Discrete Speech Units</p>

    <p><strong>Yuanchao Li</strong>, Pinzhen Chen, Peter Bell, Catherine Lai. UK Speech. 2024</p>
  </li>
  <li>
    <p>Multimodal Dyadic Impression Recognition via Listener Adaptive Cross-Domain Fusion</p>

    <p><strong>Yuanchao Li</strong>, Peter Bell, Catherine Lai. UK Speech. 2023</p>
  </li>
  <li>
    <p>Exploration of A Self-Supervised Speech Model: A Study on Emotional Corpora</p>

    <p><strong>Yuanchao Li</strong>, Yumnah Mohamied, Peter Bell, Catherine Lai. UK Speech. 2022</p>
  </li>
  <li>
    <p>An Extensible End-to-End Multitask Learning Model for Recognizing Driver States</p>

    <p><strong>Yuanchao Li</strong>. The 12th Honda R&amp;D Technical Forum. 2019</p>
  </li>
  <li>
    <p>Processing User States in Spoken Dialog Systems for Human-Robot Interaction</p>

    <p><strong>Yuanchao Li</strong>. International Design Symposium in Kyoto. 2017</p>
  </li>
  <li>
    <p>Assessment Selection for Human-Robot Interaction based on Emotion Recognition Combining Prosody and Text Information</p>

    <p><strong>Yuanchao Li</strong>, Tatsuya Kawahara. The 44th Kansai Joint Speech Seminar. 2016</p>
  </li>
</ul>

<p><strong><em>- Book Translation -</em></strong></p>
<ul>
  <li>
    <p>The Easiest Handbook for Machine Learning Project: How to Implement AI (Japanese to Chinese)</p>

    <p>いちばんやさしい機械学習プロジェクトの教本 – 人気講師が教えるAIを導入する方法</p>

    <p><a href="https://item.jd.com/13218999.html">超简单的机器学习 – 人气讲师为你讲解AI在工作中的应用</a></p>
  </li>
  <li>
    <p>The Easiest Handbook for Artificial Intelligence Business: Commercializing AI and Machine Learning (Japanese to Chinese)</p>

    <p>いちばんやさしい人工知能ビジネスの教本 – 人気講師が教えるAI・機械学習の事業化</p>

    <p><a href="https://item.jd.com/13268339.html">超简单的人工智能 – 人气讲师为你讲解AI商业应用</a></p>
  </li>
</ul>

<p><strong><em>- Media Articles -</em></strong></p>

<ul>
  <li><a href="https://syncedreview.com/2017/07/25/amazon-is-building-its-grocery-empire/">Amazon is Building its Grocery Empire</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/07/20/apple-is-in-a-dilemma-on-iphones-10-year-old-birthday/">Apple is in a Dilemma on iPhone’s 10-year-old Birthday</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/06/27/conversational-systems-a-general-review/">Conversational Systems: A General Review</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/06/09/does-fitness-data-make-the-average-person-healthier/">Does Fitness Data Make the Average Person Healthier</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/05/30/25-tweets-to-know-you-a-new-model-to-predict-personality-with-social-media/">25 Tweets to Know You: A New Model to Predict Personality with Social Media</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/05/12/why-alphago-is-not-ai/">Why AlphaGo is not AI</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/04/28/artificial-intelligence-is-the-new-electricity-andrew-ng/">Artificial Intelligence is the New Electricity – Andrew Ng</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/04/27/the-time-to-marry-ai-may-come-soon/">The Time to Marry AI May Come Soon</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/03/24/erica-the-erato-intelligent-conversational-android/">ERICA: The ERATO Intelligent Conversational Android</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/03/15/statistical-spoken-dialogue-systems-and-the-challenges-for-machine-learning/">Statistical Spoken Dialogue Systems and the Challenges for Machine Learning</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/03/14/emotional-intelligence-is-the-future-of-artificial-intelligence/">Emotional Intelligence is the Future of Artificial Intelligence</a>. Synced Review</li>
</ul>

          </section>
        </div>
      </article>
    </div>

    <script src="assets/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://raw.githubusercontent.com/yc-li20/yc-li20.github.io/'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            var totalCitation = data['citedby']
            document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>



  </body>
</html>
