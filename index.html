
<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>Jun Xue - Homepage</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content="Jun Xue">
<meta property="og:title" content="Jun Xue">


  <link rel="canonical" href="https://junxue-tech.github.io//">
  <meta property="og:url" content="https://junxue-tech.github.io//">



  <meta property="og:description" content="">









<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="assets/css/main.css">

<meta http-equiv="cleartype" content="on">
<head>
  <base target="_blank">
</head>
    <link rel="apple-touch-icon" sizes="180x180" href="images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<link rel="manifest" href="images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-TESF26FDJS"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-TESF26FDJS');
    </script>

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="#about-me">Homepage</a></li>
          
            <li class="masthead__menu-item"><a href="/#about-me">About Me</a></li>
          
            <li class="masthead__menu-item"><a href="/#-news">News</a></li>
          
            <li class="masthead__menu-item"><a href="/#-honors-and-awards">Honors and Awards</a></li>
          
            <li class="masthead__menu-item"><a href="/#-research-activities">Research Activities</a></li>
          
            <li class="masthead__menu-item"><a href="/#-talks">Talks</a></li>
          
            <li class="masthead__menu-item"><a href="/#-experiences">Experiences</a></li>
          
            <li class="masthead__menu-item"><a href="/#-education">Education</a></li>
          
            <li class="masthead__menu-item"><a href="/#-publications">Publications</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="images/android-chrome-192x192.png" class="author__avatar" alt="Jun Xue">
  </div>

  <div class="author__content">
    <h3 class="author__name">Jun Xue</h3>
    <p class="author__bio">AI Speech Algorithm Engineer @ iFLYTEK</p>
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Hefei, China</li>
      
      
      
      
        <li><a href="junxue.tech@gmail.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
        <li><a href="https://www.researchgate.net/profile/Jun-Xue-18?ev=hdr_xprf"><i class="fab fa-fw fa-researchgate" aria-hidden="true"></i> ResearchGate</a></li>
      
      
      
        <li><a href="https://www.linkedin.com/in/%E5%86%9B-%E8%96%9B-78187b2a1/"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
      
      
      
      
        <li><a href="https://github.com/JunXue-tech"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
        <li><a href="https://scholar.google.com.hk/citations?user=YxoCHH4AAAAJ&hl=zh-CN"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
      <div class="author__urls_sm">
      
      
        <a href="junxue.tech@gmail.com"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
        <a href="https://www.researchgate.net/profile/Jun-Xue-18?ev=hdr_xprf"><i class="fab fa-fw fa-researchgate" aria-hidden="true"></i></a>
      
      
      
      
      
        <a href="https://www.linkedin.com/in/%E5%86%9B-%E8%96%9B-78187b2a1/"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
        <a href="https://github.com/JunXue-tech"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>
      
      
      
      
      
  
      
      
      
      
        <a href="https://scholar.google.com.hk/citations?user=YxoCHH4AAAAJ&hl=zh-CN"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
      
      
    </div>
  </div>
</div>

  
  </div>

    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            <p><span class="anchor" id="about-me"></span></p>

<p>I am a PhD student at the <a href="https://www.cstr.ed.ac.uk">Centre for Speech Technology Research (CSTR)</a>, affiliated with the Institute for Language, Cognition and Computation (ILCC), University of Edinburgh. I am fortunate to be advised by <a href="https://homepages.inf.ed.ac.uk/clai/">Dr. Catherine Lai</a> and <a href="https://homepages.inf.ed.ac.uk/pbell1/">Prof. Peter Bell</a>, and fully funded by the School of Informatics. I was an Enrichment student at <img src="./images/ati.png" style="width: 4em;" /> and a research intern at <a href="https://www.microsoft.com/en-us/research/group/audio-and-acoustics-research-group/overview/">Audio and Acoustics Research Group</a> <img src="./images/msr.png" style="width: 5em;" />.</p>

<p>My research aims to advance spoken language technologies in real-world applications by bridging different but relevant domains such as speech &amp; language, emotion &amp; health, humans &amp; machines, etc. In particular, my work focuses on problems that hinder the broader use of spoken language technologies in the wild.</p>

<p>Before PhD study, I used to research on affective computing and human-robot interaction at Honda Innovation Lab, <a href="http://www.geminoid.jp/en/index.html">Hiroshi Ishiguro Lab ATR</a>, and <a href="http://sap.ist.i.kyoto-u.ac.jp/EN/">Speech and Audio Processing Lab</a>. I was fortunate to be advised by <a href="http://sap.ist.i.kyoto-u.ac.jp/members/kawahara/">Prof. Tatsuya Kawahara</a>, <a href="https://hb2504.utep.edu/Home/Profile?username=nigel">Prof. Nigel Ward</a>, and <a href="http://www.irc.atr.jp/~carlos/">Dr. Carlos Ishi</a>.</p>

<h1 id="-news">üî• News</h1>
<ul>
  <li><em>03.2025</em>, One paper accepted to ICME 2025. <a href="https://arxiv.org/abs/2409.15545">Addressing Emotion Bias in Music Emotion Recognition and Generation with Frechet Audio Distance</a></li>
  <li><em>02.2025</em>, Received the <strong>ICASSP Travel Grant</strong> from the IEEE Signal Processing Society.  üéâ</li>
  <li><em>12.2024</em>, Four papers accepted to ICASSP 2025 (two as the first author and two as the project leader) !</li>
  <li><em>11.2024</em>, Our special session <a href="https://sites.google.com/view/responsiblespeech/">Responsible Speech Foundation Models II</a> has been accepted to Interspeech 2025. Submit your papers and compete for the Best Paper Award!</li>
  <li><em>09.2024</em>, Our <a href="https://sites.google.com/view/spandldeteriorate">SpandLDeteriorate workshop</a> has been accepted to ACM MM Asia 2024. Looking forward to your papers!</li>
  <li><em>08.2024</em>, Three papers accepted to SLT 2024 (two as the first author and one as the co-first author)!</li>
  <li><em>04.2024</em>, Our <a href="https://sites.google.com/view/gensec-challenge">GenSEC challenge</a> has been accepted to SLT 2024. Looking forward to your papers!</li>
  <li><em>03.2024</em>, We won the <strong>1st place (and $1,000)</strong> out of 31 teams in Task 1 - Categorical Emotion Recognition at <a href="https://www.odyssey2024.org/emotion-recognition-challenge">Odyssey 2024 Emotion Recognition Challenge</a>.  üéâ</li>
  <li><em>02.2024</em>, One paper accepted to ICASSP 2024 SASB workshop: <a href="https://arxiv.org/abs/2402.02617">Layer-Wise Analysis of Self-Supervised Acoustic Word Embeddings: A Study on Speech Emotion Recognition</a></li>
  <li><em>01.2024</em>, Our special session <a href="https://sites.google.com/view/responsiblespeech/">Responsible Speech Foundation Models</a> has been accepted to Interspeech 2024. Looking forward to your papers!</li>
  <li><em>11.2023</em>, <a href="https://github.com/microsoft/fadtk">Microsoft FADTK</a>, a Frechet audio distance toolkit has been released, to which I contributed its speech models.</li>
  <li><em>09.2023</em>, Received the <strong>IEEE SPS Scholarship</strong> from the IEEE Signal Processing Society.  üéâ</li>
  <li><em>09.2023</em>, Received the <strong>Outstanding Paper Award</strong> at the SAI workshop, ACII 2023, MIT Media Lab.  üéâ</li>
  <li><em>06.2023</em>, Our grant proposal (as Co-Investigator) <strong>Development of A Human-Centric Elderly Driving Education System</strong> has been accepted by the Inter-University Research Institute Corporation, Research Organization of Information and Systems.</li>
  <li><em>03.2023</em>, Received the <strong>Gary Marsden Travel Award</strong> from ACM SIGCHI.  üéâ</li>
</ul>

<h1 id="-honors-and-awards">üéñ Honors and Awards</h1>
<ul>
  <li><em>2025</em>, ICASSP Travel Grant, IEEE Signal Processing Society</li>
  <li><em>2024</em>, 3rd Place Award, 3-Min Thesis Competition, Students of Society for Affective Science</li>
  <li><em>2024</em>, 1st Place Award, Odyssey 2024 Emotion Recognition Challenge</li>
  <li><em>2023</em>, IEEE SPS Scholarship, IEEE Signal Processing Society</li>
  <li><em>2023</em>, Outstanding Paper Award, SAI workshop, ACII 2023</li>
  <li><em>2023</em>, Gary Marsden Travel Award, ACM SIGCHI</li>
  <li><em>2022</em>, Enrichment Student Award, Alan Turing Institute</li>
  <li><em>2021</em>, Fully-Funded PhD Scholarship, University of Edinburgh</li>
  <li><em>2016</em>, Seiwa International Scholarship, Kyoto University</li>
  <li><em>2013</em>, 3rd Class Academic Excellence Scholarship, NUPT</li>
</ul>

<h1 id="-research-activities">üíª Research Activities</h1>
<p><strong><em>- Organizing Committee -</em></strong></p>
<ul>
  <li><a href="https://sites.google.com/view/spandldeteriorate">Workshop on Multi-Biological Sensing Data for Speech and Language Deterioration Prediction</a> at ACM MM Asia 2024</li>
  <li><a href="https://sites.google.com/view/gensec-challenge">GenSEC (old name: GenASR) challenge</a> at SLT 2024</li>
  <li>Special Session at Interspeech 2024: <a href="https://sites.google.com/view/responsiblespeech/">Responsible Speech Foundation Models</a></li>
  <li><a href="https://sites.google.com/view/uk-sigmm">UK Special Interest Group in Speech-Based Multimodal Information Processing</a></li>
  <li><a href="https://conferences.inf.ed.ac.uk/ukspeech2022/">UK Speech 2022</a></li>
</ul>

<p><strong><em>- Program Committee -</em></strong></p>
<ul>
  <li>Interspeech Young Female Researchers in Speech Workshop <a href="https://sites.google.com/view/yfrsw-2024/">2024</a>, <a href="https://sites.google.com/view/yfrsw-2022/">2022</a></li>
  <li><a href="https://icmi.acm.org/2021/index.php?id=cfdc">ICMI 2021 Doctoral Consortium</a></li>
</ul>

<p><strong><em>- Journal Review -</em></strong></p>
<ul>
  <li>IEEE Transactions on Affective Computing (2)</li>
  <li>Computer Speech and Language (2)</li>
  <li>IEEE Transactions on Audio, Speech and Language Processing (1)</li>
  <li>Speech Communication (1)</li>
  <li>Computers in Human Behavior (1)</li>
  <li>Journal of Rehabilitation and Assistive Technologies Engineering (1)</li>
  <li>Pattern Analysis and Applications (1)</li>
  <li>Robotics and Autonomous Systems (1)</li>
</ul>

<p><strong><em>- Conference Review -</em></strong></p>
<ul>
  <li>ICASSP‚Äô23-25, Interspeech‚Äô23-25, ICME‚Äô25, IJCNN‚Äô25, ASRU‚Äô23, SLT‚Äô22-24, UK Speech‚Äô22</li>
  <li>Interspeech Young Female Researchers in Speech Workshop‚Äô24 &amp; 22</li>
  <li>CHI‚Äô23 Late-Breaking Work</li>
  <li>IJCLR‚Äô23 CogAI Workshop</li>
  <li>ICMI‚Äô21 Late-Breaking Report &amp; Doctoral Consortium</li>
  <li>HRI‚Äô20 Late-Breaking Report</li>
</ul>

<p><strong><em>- Organizations &amp; Communities -</em></strong></p>
<ul>
  <li>ACM, AAAC, ISCA, IEEE, IEEE Signal Processing Society, SIGCHI, UK Speech, UK-SIGMM, Alan Turing Institute</li>
</ul>

<h1 id="-talks">üéô Talks</h1>
<ul>
  <li><em>12.2024</em>, ‚ÄúOpportunities and Challenges of Language Emotion in Real Applications: LLMs, Multimodal Incongruity, and Human-Robot Interaction‚Äù. Tsinghua Laboratory of Brain and Intelligence, Tsinghua University (host: Prof. Dan Zhang)</li>
  <li><em>12.2024</em>, ‚ÄúMulti-view Cognitive State Detection Based on Pre-trained Speech and Language Models‚Äù. Speech and Audio Technology Lab, Tsinghua University (host: Prof. Wei-Qiang Zhang)</li>
  <li><em>03.2024</em>, ‚ÄúOpportunities and Challenges of Speech Emotion Recognition in the Era of Foundation Models‚Äù. Center for Interdisciplinary Research in Language Sciences, University of Science and Technology of China (host: Prof. Jiahong Yuan)</li>
  <li><em>11.2020</em>, ‚ÄúAffective Human-Robot Interaction‚Äù. Cognitive Developmental Robotics Lab, University of Tokyo (host: Prof. Yukie Nagai)</li>
</ul>

<h1 id="-grants">üí∞ Grants</h1>
<ul>
  <li><em>06.2023</em>, ‚ÄúDevelopment of A Human-Centric Elderly Driving Education System‚Äù, Co-Investigator, ¬•800,000. Strategic Research Project ‚Äú2023-SRP-06‚Äù, Research Organization of Information and Systems</li>
</ul>

<h1 id="-experiences">üëî Experiences</h1>
<p><strong><em>- Teaching -</em></strong></p>
<ul>
  <li>TA (Coursework marker), Automatic Speech Recognition, University of Edinburgh, 2023 &amp; 2024</li>
  <li>TA (Tutor, demonstrator, and project marker), System Design Project, University of Edinburgh, 2023</li>
  <li>TA (Coursework and exam marker), Machine Learning, University of Edinburgh, 2022 &amp; 2024</li>
</ul>

<p><strong><em>- Supervision -</em></strong></p>
<ul>
  <li>
    <p>Cross-lingual Speech Emotion Recognition and Speech Emotion Diarisation: A Comparative Study between Humans and Machines</p>

    <p>Zhichen Han, MSc dissertation 2024/25 (Distinction), University of Edinburgh</p>
  </li>
  <li>
    <p>Revisiting the Shared Suprasegmental Acoustics Between Emotional Speech and Song through Self-Supervised Learning Models</p>

    <p>Yujia Sun, MSc dissertation 2024/25 (Distinction), University of Edinburgh</p>
  </li>
  <li>
    <p>Layerwise Analysis of HuBERT Acoustic Word Embeddings in the Context of Speech Emotion Recognition</p>

    <p>Alexandra Saliba, MSc dissertation 2023/24 (Distinction), University of Edinburgh</p>
  </li>
  <li>
    <p>Hierarchical Cross-Modal Transformer and A Study of Cross-Modal Attention for Affective Computing</p>

    <p>Yaoting Wang, MSc dissertation 2022/23 (Distinction), University of Edinburgh</p>
  </li>
  <li>
    <p>A Cross-Domain Study of Crossmodal Attention Based Multimodal Emotion Recognition</p>

    <p>Junling Liu, MSc dissertation 2021/22, University of Edinburgh</p>
  </li>
</ul>

<p><strong><em>- Working -</em></strong></p>
<ul>
  <li>Research Intern, Microsoft Research Audio and Acoustics Group</li>
  <li>Researcher, Honda R&amp;D Innovation Lab</li>
  <li>R&amp;D Engineer, NTT Data R&amp;D headquarters</li>
  <li>Student Researcher, ERATO ISHIGURO Symbiotic HRI Project, ATR</li>
</ul>

<h1 id="-education">üìñ Education</h1>
<ul>
  <li>Ph.D. Candidate, Informatics, University of Edinburgh</li>
  <li>M.Sc., Intelligence Science and Technology, Kyoto University</li>
  <li>B.Eng., Electronic and Information Engineering, Nanjing University of Posts and Telecommunications</li>
</ul>

<h1 id="-publications">üìù Publications</h1>

<p><strong><em>- Papers -</em></strong></p>
<ul>
  <li>See my <a href="https://scholar.google.com/citations?hl=en&amp;user=WYSrzUsAAAAJ&amp;view_op=list_works&amp;sortby=pubdate">Google Scholar</a></li>
</ul>

<p><strong><em>- Patents -</em></strong></p>
<ul>
  <li><a href="https://patents.google.com/patent/US11107464B2/en">Feeling estimation device, feeling estimation method, and storage medium</a>. US11107464B2, JP2020091302A, CN111341349A</li>
  <li><a href="https://patents.google.com/patent/US11443759B2/en">Information processing apparatus, information processing method, and storage medium</a>. US11443759B2, JP2021026130A, CN112349301A</li>
  <li><a href="https://patents.google.com/patent/US11710499B2/en">Information-processing device, vehicle, computer-readable storage medium, and information-processing method</a>. US11710499B2, JP2021124642A, CN113221933A</li>
</ul>

<p><strong><em>- Technical Reports -</em></strong></p>
<ul>
  <li>
    <p>Crossmodal ASR Error Correction with Discrete Speech Units</p>

    <p><strong>Yuanchao Li</strong>, Pinzhen Chen, Peter Bell, Catherine Lai. UK Speech. 2024</p>
  </li>
  <li>
    <p>Multimodal Dyadic Impression Recognition via Listener Adaptive Cross-Domain Fusion</p>

    <p><strong>Yuanchao Li</strong>, Peter Bell, Catherine Lai. UK Speech. 2023</p>
  </li>
  <li>
    <p>Exploration of A Self-Supervised Speech Model: A Study on Emotional Corpora</p>

    <p><strong>Yuanchao Li</strong>, Yumnah Mohamied, Peter Bell, Catherine Lai. UK Speech. 2022</p>
  </li>
  <li>
    <p>An Extensible End-to-End Multitask Learning Model for Recognizing Driver States</p>

    <p><strong>Yuanchao Li</strong>. The 12th Honda R&amp;D Technical Forum. 2019</p>
  </li>
  <li>
    <p>Processing User States in Spoken Dialog Systems for Human-Robot Interaction</p>

    <p><strong>Yuanchao Li</strong>. International Design Symposium in Kyoto. 2017</p>
  </li>
  <li>
    <p>Assessment Selection for Human-Robot Interaction based on Emotion Recognition Combining Prosody and Text Information</p>

    <p><strong>Yuanchao Li</strong>, Tatsuya Kawahara. The 44th Kansai Joint Speech Seminar. 2016</p>
  </li>
</ul>

<p><strong><em>- Book Translation -</em></strong></p>
<ul>
  <li>
    <p>The Easiest Handbook for Machine Learning Project: How to Implement AI (Japanese to Chinese)</p>

    <p>„ÅÑ„Å°„Å∞„Çì„ÇÑ„Åï„Åó„ÅÑÊ©üÊ¢∞Â≠¶Áøí„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆÊïôÊú¨ ‚Äì ‰∫∫Ê∞óË¨õÂ∏´„ÅåÊïô„Åà„ÇãAI„ÇíÂ∞éÂÖ•„Åô„ÇãÊñπÊ≥ï</p>

    <p><a href="https://item.jd.com/13218999.html">Ë∂ÖÁÆÄÂçïÁöÑÊú∫Âô®Â≠¶‰π† ‚Äì ‰∫∫Ê∞îËÆ≤Â∏à‰∏∫‰Ω†ËÆ≤Ëß£AIÂú®Â∑•‰Ωú‰∏≠ÁöÑÂ∫îÁî®</a></p>
  </li>
  <li>
    <p>The Easiest Handbook for Artificial Intelligence Business: Commercializing AI and Machine Learning (Japanese to Chinese)</p>

    <p>„ÅÑ„Å°„Å∞„Çì„ÇÑ„Åï„Åó„ÅÑ‰∫∫Â∑•Áü•ËÉΩ„Éì„Ç∏„Éç„Çπ„ÅÆÊïôÊú¨ ‚Äì ‰∫∫Ê∞óË¨õÂ∏´„ÅåÊïô„Åà„ÇãAI„ÉªÊ©üÊ¢∞Â≠¶Áøí„ÅÆ‰∫ãÊ•≠Âåñ</p>

    <p><a href="https://item.jd.com/13268339.html">Ë∂ÖÁÆÄÂçïÁöÑ‰∫∫Â∑•Êô∫ËÉΩ ‚Äì ‰∫∫Ê∞îËÆ≤Â∏à‰∏∫‰Ω†ËÆ≤Ëß£AIÂïÜ‰∏öÂ∫îÁî®</a></p>
  </li>
</ul>

<p><strong><em>- Media Articles -</em></strong></p>

<ul>
  <li><a href="https://syncedreview.com/2017/07/25/amazon-is-building-its-grocery-empire/">Amazon is Building its Grocery Empire</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/07/20/apple-is-in-a-dilemma-on-iphones-10-year-old-birthday/">Apple is in a Dilemma on iPhone‚Äôs 10-year-old Birthday</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/06/27/conversational-systems-a-general-review/">Conversational Systems: A General Review</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/06/09/does-fitness-data-make-the-average-person-healthier/">Does Fitness Data Make the Average Person Healthier</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/05/30/25-tweets-to-know-you-a-new-model-to-predict-personality-with-social-media/">25 Tweets to Know You: A New Model to Predict Personality with Social Media</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/05/12/why-alphago-is-not-ai/">Why AlphaGo is not AI</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/04/28/artificial-intelligence-is-the-new-electricity-andrew-ng/">Artificial Intelligence is the New Electricity ‚Äì Andrew Ng</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/04/27/the-time-to-marry-ai-may-come-soon/">The Time to Marry AI May Come Soon</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/03/24/erica-the-erato-intelligent-conversational-android/">ERICA: The ERATO Intelligent Conversational Android</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/03/15/statistical-spoken-dialogue-systems-and-the-challenges-for-machine-learning/">Statistical Spoken Dialogue Systems and the Challenges for Machine Learning</a>. Synced Review</li>
  <li><a href="https://syncedreview.com/2017/03/14/emotional-intelligence-is-the-future-of-artificial-intelligence/">Emotional Intelligence is the Future of Artificial Intelligence</a>. Synced Review</li>
</ul>

          </section>
        </div>
      </article>
    </div>

    <script src="assets/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://raw.githubusercontent.com/yc-li20/yc-li20.github.io/'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            var totalCitation = data['citedby']
            document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>



  </body>
</html>
